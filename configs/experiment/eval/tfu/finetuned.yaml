# @package _global_

defaults:
  - /data: unlearn
  - override /model: Llama-3.2-1B-Instruct
  - override /eval: tofu

forget_split: forget10
holdout_split: holdout10
retain_logs_path: null

model:
  model_args:
    pretrained_model_name_or_path: open-unlearning/tofu_Llama-3.2-1B-Instruct_full
  model_handler: TFULlamaForCausalLM
  w: 1

tfu:
  # FAISS_model: "BAAI/bge-base-en-v1.5"
  # search_type: "similarity"
  # search_topk: 1
  help_model:
    pretrained_model_name_or_path: "./saves/finetune/SAMPLE_TRAIN_forget"
    attn_implementation: "flash_attention_2"
    torch_dtype: "bfloat16"
  activation_method: naive
  activation_threshold: 0.55

eval:
  tofu:
    forget_split: ${forget_split}
    holdout_split: ${holdout_split}
    retain_logs_path: ${retain_logs_path}

task_name: ???